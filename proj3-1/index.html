<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184 Pathtracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2022</h1>
<h1 align="middle">Project 3: Pathtracer</h1>
<h2 align="middle">Justin Radatti and Dakshina Palasamudrum, CS184-0xda-ju</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
<p> 
This project was very challenging, engaging, and time consuming. Although it was difficult, it was one of the most interesting projects I have ever done because it is so cool to slightly change an algorithm and see how it completely affects the way the image was rendered. Throughout this project, we implemented Ray Generation and Scene Intersection, Bounding Volume Hierarchy, Direct Illumination, Global Illumination, and adaptive sampling. In the first part, we handled the construction of rays and used the Möller Trumbore Algorithm to test for intersections with triangles and the algorithm described in lecture for sphere intersections. Next, we implemented BVH optimization to speed up our rendering process. It was amazing to watch the render times go from 10s of seconds to less than a tenth of a second. After that, we moved on to illuminating our scene. Starting with direct illumination using zero bounces and one bounces of light. For this part, we used a hemisphere sampler as well as a sampler around the area of each light and it was very cool to see the noise decrease using the latter sampler. Next, we implemented global illumination using a recursive algorithm to trace at least one bounce of light, up to max_ray_depth bounces of light, and terminating randomly with Russian Roulette. Finally, we added adaptive sampling to minimize the number of samples needed per pixel while still resulting in a smooth image by periodically checking whether or not the pixel has converged. This project was illuminating to the fascinating world of computer renders and was well-worth the time and frustrations; I cannot wait for 3.2.
</p>

<h3 align="middle">Part 1: Ray Generation and Scene Intersection</h3>

<p>In part 1, we generated camera rays, pixel samples, and implemented ray-triangle and ray-sphere intersection. In terms of ray generation, after generating rays for each pixel, we had to look for intersection points between the rays and surfaces in the image. We did this by transforming the image coordinates into camera space, generated the ray in the camera space, and transformed it in the world space. In task 3 of part 1 we implemented ray-triangle intersection using the Möller Trumbore Algorithm. The basic idea behind ray intersection is to test if a point is inside the triangle, and one way to optimize this is to use this algorithm. We implemented this algorithm by setting up a system of equations where we found the ray’s t-value and the barycentric coordinates, by doing this, we are able to detect if the ray hit the triangle. After setting up the equations, we restricted the range of the t-values by ensuring that our barycentric coordinate values were greater than 0, and  that our t-values were greater than min_t and less than max_t. Finally, we set max_t = t so that we can grab the nearest intersection. This essentially tests if there is an intersection. Once an intersection takes place, we then record the intersection and update the data. 
</p>


<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption align="middle">Spheres</figcaption>
      </td>
      <td>
        <img src="images/CBgems.png" align="middle" width="400px"/>
        <figcaption align="middle">Gems</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<h3 align="middle">Part 2: Bounding Volume Hierarchy </h3>

<p>Our BVH algorithm recursively splits the scene primitives into BVHNodes. The construct_bvh() function takes in two iterators, start and end, as parameters and first finds the bounding box that encapsulates all the primitives in the range(inclusively) and makes a new BVHNode called ‘node’. If the number of primitives is small enough, we set the start and end fields of ‘node’ and return it as the base case. If the bounding box wasn’t small enough, we need to partition the primitives into a left and a right group. To do so, we find the longest axis of the bounding box using the BBox.extent field and then find the average of centroids along that axis. Next, we partition the primitives into two groups based on their centroid location along the longest axis relative to the average centroid location. In order to avoid infinite recursion, we then check if there is at least one primitive in each group and if there isn’t we adjust the iterators accordingly. Finally, we recurse one the construct_bvh() function and set the result to node->left and node->right. 
</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/CBlucy.png" align="middle" width="400px"/>
        <figcaption align="middle">Lucy(133796 primitives, 0.6120 seconds)</figcaption>
      </td>
      <td>
        <img src="images/maxplanck.png" align="middle" width="400px"/>
        <figcaption align="middle">MaxPlanck(50801 primitives, 0.0575 seconds)</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<p> I used cow.dae and beetle.dae to compare the rendering times between using and not using the Bounded Volume Hierarchy optimization. Cow.dae and beetle.dae consist of 5856 and 7558 primitives, respectively. Without using BVH, the beetle took 9.47 seconds and the cow took 6.69 seconds on my local machine. The cow and beetle averaged 1506 and 1947 intersection tests per ray, respectively. Alternatively, using BVH, cow.dae took 0.037 seconds and the beetle took 0.034 seconds. The BVH optimization decreases the intersection tests per ray to an average of 4 and 3 for cow and beetle, respectively. 
</p>

<h3 align="middle">Part 3: Direct Illumination </h3>
 
<p>The estimate_direct_lighting_hemisphere() function takes in a ray ‘r’ and an intersection ‘isect’ with the goal of finding the reflectance at the hotpoint ’hit_p’. To do so, we use Monte Carlo estimation to average the reflectances of ‘num_samples’ sample and scale by the probability of each sample, (1/(2*PI)). For each sample, we cast a different ray ‘ray’ from ‘hit_p’ and check if it intersects with a scene primitive, storing the intersection in ‘new_isect’. If ‘ray’ collides with something, we measure the radiance, ‘radiance’,  with zero_bounce_radiance() using ’new_isect’ and measure the ratio ‘f’ by calling ‘insect.bsdf->f()’. Then, we measure the reflectance at ‘hit_p’ by doing “(radiance * f * cos_theta(w_in)) / pdf” where cos_theta(w_in) is the angle between ‘ray’ and the surface normal.  
</p>

<p>The estimate_direct_lighting_importance() function Follows a similar process as estimate_direct_lighting_hemisphere() except the sample directions are not taken uniformly around a hemisphere. Instead, the samples are taken in the direction of the lights in the scene. If the sample intersects an object in the scene, the object is between the light and 'hit_p' which implies there is a shadow at 'hit_p'. Thus, we only add to 'L_out' when the ray from 'hit_p' in the sampled direction does not hit an object before the light. More specifically, the algorithm is as follows. For every light in the scene, we take ’ns_area_light’ samples using SceneLight::sample_L(), which samples the light between the light source and the point ‘hit_p’. Next, we add the sampled light to the total light ‘L_out’ and scale by the pdf returned by sample_L(). Finally, we divide 
‘L_out’ by ’ns_area_light’ to complete our Monte Carlo estimation.
</p>

<p> 
Examples of Hemisphere light sampling below:
</p>


<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/CBbunny_H_16_8.png" align="middle" width="400px"/>
        <figcaption align="middle">Hemisphere sampling. 64 samples. </figcaption>
      </td>
      <td>
        <img src="images/hemisphere3.png" align="middle" width="400px"/>
        <figcaption align="middle">Hemisphere sampling. 64 samples. </figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/hemisphere1.png" align="middle" width="400px"/>
        <figcaption align="middle">Hemisphere sampling. 64 samples. </figcaption>
      </td>
      <td>
        <img src="images/hemisphere2.png" align="middle" width="400px"/>
        <figcaption align="middle">Hemisphere sampling. 64 samples. </figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<p> 
Examples of Importance light sampling below:
</p>


<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/bunny_1_1.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 1 samples. </figcaption>
      </td>
      <td>
        <img src="images/importance3.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 64 samples. </figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/importance1.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 64 samples. </figcaption>
      </td>
      <td>
        <img src="images/importance2.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 64 samples. </figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<p> 
Using Importance Sampling to render with 1, 4, 16, and 64 light rays and with 1 sample per pixel shown below. The noise in the soft shadows decreases as we increase the number of rays.
</p>


<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/dragon_1_1.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 1 light rays with 1 sample per pixel. </figcaption>
      </td>
      <td>
        <img src="images/dragon_1_4.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 4 light rays with 1 sample per pixel. </figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/dragon_1_16.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 16 light rays with 1 sample per pixel. </figcaption>
      </td>
      <td>
        <img src="images/dragon_1_64.png" align="middle" width="400px"/>
        <figcaption align="middle">Light sampling. 64 light rays with 1 sample per pixel. </figcaption>
      </td>
    </tr>
  </table>
</div>

<p> 
The main difference between estimate_direct_lighting_hemisphere() and estimate_direct_lighting_importance() is that importance lighting has less noise. This is due to the sampling method. Since hemisphere estimation samples uniformly around a hemisphere, there is more margin of error to miss the light sources compared to importance lighting. In importance lighting, we only sample in the direction of the light. Thus, the accuracy is higher and there is less false negatives. The images above clearly show the advantage of importance sampling; with 1 sample and 1 ray hemisphere sampling is unrecognizable while importance sampling is relatively clear. All in all, the images rendered with hemisphere sampling are more grainy than the images rendered with Importance sampling.
</p>

<h3 align="middle">Part 4: Global Illumination </h3>

<p>Our ‘at_least_one_bounce_radiance()’ function implements indirect lighting by recursively summing the lighting of ‘max_ray_depth’ bounces through the scene(or sometimes less due to Russian Roulette). Our base case is if ‘r.depth <= 1’, we return the zero vector. Otherwise, we add the radiance using ‘one_bounce_radiance()’ with the given ‘ray’ and ‘isect’ to L_out. Next, we use the ‘BSDF::sample_f()’ function to get the ratio of the bsdf at ‘hit_p’, as well as the pdf and sampled direction ‘wi’. We then shoot a ray, ‘ray’, into the scene in direction ‘wi’ and set its depth to ‘r.depth - 1’. If the ray intersects with something, we follow the ray and calculate the radiance at the next hit point. More concretely, if the ray, ‘ray’, intersects a scene primitive at ‘new_isect’, we increment L_out by “at_least_one_bounce_radiance(ray, new_isect) * sample * cos_theta(wi) / pdf / cpdf” where sample is the return value of ‘sample_f()’ and cpdf is set to 0.7. However there is one more wrinkle, we only add the previous value to L_out with probability 0.3. To implement this we used the ‘coin_flip()’’ function and an if statement. 
</p>

<p>Pictured below are two scenes rendered with Global Illumination:</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/spheres1024.png" align="middle" width="400px"/>
        <figcaption align="middle">Spheres rendered with 1024 samples per pixel(globel illumination)</figcaption>
      </td>
      <td>
        <img src="images/bunny1024.png" align="middle" width="400px"/>
        <figcaption align="middle">Bunny rendered with 1024 samples per pixel(global illumination)</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<p>Pictured below is CBspheres_lambertian.dae rendered with <b>only</b> direct illumination(left) and <b>only</b> indirect illumination(right)</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/spheresOnlyDirect.png" align="middle" width="400px"/>
        <figcaption align="middle">Spheres rendered with 1024 samples per pixel(only direct illumination)</figcaption>
      </td>
      <td>
        <img src="images/spheresOnlyIndirect.png" align="middle" width="400px"/>
        <figcaption align="middle">Spheres rendered with 1024 samples per pixel(only indirect illumination)</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<p>Pictured below is CBbunny.dae rendered with max_ray_depth set to 0,1,2,3, and 100 and rendered with 1024 samples per pixel.</p>


<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/bunnyMRD0.png" align="middle" width="400px"/>
        <figcaption align="middle">max_ray_depth = 0</figcaption>
      </td>
      <td>
        <img src="images/bunnyMRD1.png" align="middle" width="400px"/>
        <figcaption align="middle">max_ray_depth = 1</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/bunnyMRD2.png" align="middle" width="400px"/>
        <figcaption align="middle">max_ray_depth = 2</figcaption>
      </td>
      <td>
        <img src="images/bunnyMRD3.png" align="middle" width="400px"/>
        <figcaption align="middle">max_ray_depth = 3</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/bunnyMRD100.png" align="middle" width="400px"/>
        <figcaption align="middle">max_ray_depth = 100</figcaption>
      </td>
    </tr>
  </table>
</div>

<p>Pictured below is bench.dae rendered with 4 light rays and 1, 2, 4, 8, 16, 64, and 1024 samples per pixel.</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/benchSR1.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 1</figcaption>
      </td>
      <td>
        <img src="images/benchSR2.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 2</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/benchSR4.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 4</figcaption>
      </td>
      <td>
        <img src="images/benchSR8.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 8</figcaption>
      </td>    
    </tr>
    <br>
    <tr>
      <td>
        <img src="images/benchSR16.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 16</figcaption>
      </td>
      <td>
        <img src="images/benchSR64.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 64</figcaption>
      </td>
    </tr>
    <br>
    <td>
        <img src="images/benchSR1024.png" align="middle" width="400px"/>
        <figcaption align="middle">samples-per-pixel = 1024</figcaption>
      </td>

  </table>
</div>


<h3 align="middle">Part 5: Adaptive Sampling </h3>

<p>Adaptive sampling is an algorithm to minimize the number of samples we need to take while still getting a smooth render. To do so, we use the same raytrace_pixel algorithm from part 1, with a few slight adjustments. Every samplesPerBatch sample we take, we check if the pixel has converged. To do so, we calculate the mean and standard deviation using s1 and s2 where s1 is the sum of illuminance from all samples 1-n (n is the number of samples already taken) and s2 is the sum of illuminance squared from all samples 1-n. Our calculation for mean is s1 / n and our calculation for standard deviation is sqrt((1.0 / (n - 1.0)) * (s2 - (s1^2 / n))). Then, we calculate the convergence of the current pixel as I =  1.96 * (sd / sqrt(n)) and check if I <= maxTolerance * mean. If it is, we stop taking samples and set the radiance at the current pixel to the sum of all samples scaled by the number of samples taken. If it is not, we add the current sample to the sum of all samples and move on to the next sample. 
</p>

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="images/bunnyP5_rate.png" align="middle" width="400px"/>
        <figcaption align="middle">Adaptive sampling with 2048 samples per pixel</figcaption>
      </td>
      <td>
        <img src="images/bunnyP5.png" align="middle" width="400px"/>
        <figcaption align="middle">Adaptive sampling with 2048 samples per pixel</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>

<p>Throughout the entirety of this project, we learned a significant amount about ray generation, global illumination, and adaptive sampling. Using zoom, we were able to share one of our screens and were able to collaborate effectively. This went really well because we were also able to edit each other’s screens so that we could work at the same time. We found this way of collaboration really useful because we were able to ask each other questions and understand the concepts better allowing us to learn more. For instance, while implementing ray-triangle intersection, we looked at lecture slides together and used google in order to figure out Möller’s Algorithm. Overall, we had a great experience!
</p>



<a href="https://cal-cs184-student.github.io/sp22-project-webpages-JDRadatti/">https://cal-cs184-student.github.io/sp22-project-webpages-JDRadatti/</a>

